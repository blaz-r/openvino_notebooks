{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with OpenVINOâ„¢ using Segmenter\n",
    "\n",
    "Semantic segmentation is a difficult computer vision problem with many applications such as autonomous driving, robotics, augmented reality, and many others.\n",
    "Its goal is to assign labels to each pixel according to the object it belongs to, creating so-called segmentation masks.\n",
    "To properly assign this label, the model needs to consider the local as well as global context of the image.\n",
    "This is where transformers offer their advantage as they work well in capturing global context.\n",
    "\n",
    "Segmenter is based on Vision Transformer working as an encoder, and Mask Transformer working as a decoder.\n",
    "With this configuration, it achieves good results on different datasets such as ADE20K, Pascal Context, and Cityscapes.\n",
    "It works as shown in the diagram below, by taking the image, splitting it into patches, and then encoding these patches.\n",
    "Mask transformer combines encoded patches with class masks and decodes them into a segmentation map as the output, where each pixel has a label assigned to it.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/24582831/148507554-87eb80bd-02c7-4c31-b102-c6141e231ec8.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "More about the model and its details can be found in the following paper:\n",
    "[Segmenter: Transformer for Semantic Segmentation](https://arxiv.org/abs/2105.05633) or in the [repository](https://github.com/rstrudel/segmenter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how to convert and use Segmenter in OpenVINO, this notebook consists of the following steps:\n",
    "\n",
    "* Preparing PyTorch Segmenter model\n",
    "* Preparing preprocessing and visualization function\n",
    "* Validating inference of original model\n",
    "* Converting PyTorch model to ONNX\n",
    "* Converting ONNX to OpenVINO IR\n",
    "* Validating inference of converted model\n",
    "* Validating converted model on a subset of ADE20K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and prepare PyTorch model\n",
    "\n",
    "First thing we'll need to do is clone [repository](https://github.com/rstrudel/segmenter) containing model and helper functions. We will use Tiny model with mask transformer, that is Seg-T-Mask/16. There are also better, but much larger models available in linked repo. This model is pretrained on [ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/) dataset used for segmentation.\n",
    "\n",
    "PyTorch models are usually instance of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary containing model weights.\n",
    "Typical steps to get model are therefore:\n",
    "\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode\n",
    "\n",
    "In our case, the code from repository already contains functions that create model and load weights, but we will need to download config and trained wight (checkpoint) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need timm and mmsegmentation module, to use segmenter repo\n",
    "!pip install timm mmsegmentation einops -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will clone the Segmenter repo and then download weights and config for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenter repo already cloned\n",
      "C:\\Users\\blazr\\Documents\\code\\gsoc\\openvino_notebooks\\notebooks\\236-segmenter-semantic-segmentation\\segmenter\n"
     ]
    }
   ],
   "source": [
    "# clone Segmenter repo\n",
    "if not Path(\"segmenter\").exists():\n",
    "    !git clone https://github.com/rstrudel/segmenter\n",
    "else:\n",
    "    print(\"Segmenter repo already cloned\")\n",
    "%cd segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'model\\checkpoint.pth' already exists.\n",
      "'model\\variant.yml' already exists.\n"
     ]
    }
   ],
   "source": [
    "# download config and pretrained model weights\n",
    "# here we use tiny model, there are also better but larger models available in repository\n",
    "WEIGHTS_LINK = \"https://www.rocq.inria.fr/cluster-willow/rstrudel/segmenter/checkpoints/ade20k/seg_tiny_mask/checkpoint.pth\"\n",
    "CONFIG_LINK = \"https://www.rocq.inria.fr/cluster-willow/rstrudel/segmenter/checkpoints/ade20k/seg_tiny_mask/variant.yml\"\n",
    "\n",
    "MODEL_DIR = Path(\"model/\")\n",
    "DATA_DIR = Path(\"data/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(WEIGHTS_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "download_file(CONFIG_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "\n",
    "WEIGHT_PATH = MODEL_DIR / \"checkpoint.pth\"\n",
    "CONFIG_PATH = MODEL_DIR / \"variant.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PyTorch model\n",
    "\n",
    "We will now use already provided helper functions from repository to initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Segmenter(\n  (encoder): VisionTransformer(\n    (patch_embed): PatchEmbedding(\n      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n    (blocks): ModuleList(\n      (0): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n      )\n      (1): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.009)\n      )\n      (2): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.018)\n      )\n      (3): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.027)\n      )\n      (4): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.036)\n      )\n      (5): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.045)\n      )\n      (6): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.055)\n      )\n      (7): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.064)\n      )\n      (8): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.073)\n      )\n      (9): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.082)\n      )\n      (10): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.091)\n      )\n      (11): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): DropPath(drop_prob=0.100)\n      )\n    )\n    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n    (head): Linear(in_features=192, out_features=1000, bias=True)\n    (pre_logits): Identity()\n  )\n  (decoder): MaskTransformer(\n    (blocks): ModuleList(\n      (0): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.1, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (drop_path): Identity()\n      )\n      (1): Block(\n        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=192, out_features=576, bias=True)\n          (attn_drop): Dropout(p=0.1, inplace=False)\n          (proj): Linear(in_features=192, out_features=192, bias=True)\n          (proj_drop): Dropout(p=0.1, inplace=False)\n        )\n        (mlp): FeedForward(\n          (fc1): Linear(in_features=192, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=768, out_features=192, bias=True)\n          (drop): Dropout(p=0.1, inplace=False)\n        )\n        (drop_path): Identity()\n      )\n    )\n    (proj_dec): Linear(in_features=192, out_features=192, bias=True)\n    (decoder_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n    (mask_norm): LayerNorm((150,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from segm.model.factory import load_model\n",
    "\n",
    "pytorch_model, config = load_model(WEIGHT_PATH)\n",
    "# put model into eval mode, to set it for inference\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
