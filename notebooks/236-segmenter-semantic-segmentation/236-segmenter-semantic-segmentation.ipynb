{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with OpenVINOâ„¢ using Segmenter\n",
    "\n",
    "Semantic segmentation is a difficult computer vision problem with many applications such as autonomous driving, robotics, augmented reality, and many others.\n",
    "Its goal is to assign labels to each pixel according to the object it belongs to, creating so-called segmentation masks.\n",
    "To properly assign this label, the model needs to consider the local as well as global context of the image.\n",
    "This is where transformers offer their advantage as they work well in capturing global context.\n",
    "\n",
    "Segmenter is based on Vision Transformer working as an encoder, and Mask Transformer working as a decoder.\n",
    "With this configuration, it achieves good results on different datasets such as ADE20K, Pascal Context, and Cityscapes.\n",
    "It works as shown in the diagram below, by taking the image, splitting it into patches, and then encoding these patches.\n",
    "Mask transformer combines encoded patches with class masks and decodes them into a segmentation map as the output, where each pixel has a label assigned to it.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/24582831/148507554-87eb80bd-02c7-4c31-b102-c6141e231ec8.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "More about the model and its details can be found in the following paper:\n",
    "[Segmenter: Transformer for Semantic Segmentation](https://arxiv.org/abs/2105.05633) or in the [repository](https://github.com/rstrudel/segmenter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how to convert and use Segmenter in OpenVINO, this notebook consists of the following steps:\n",
    "\n",
    "* Preparing PyTorch Segmenter model\n",
    "* Preparing preprocessing and visualization function\n",
    "* Validating inference of original model\n",
    "* Converting PyTorch model to ONNX\n",
    "* Converting ONNX to OpenVINO IR\n",
    "* Validating inference of converted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and prepare PyTorch model\n",
    "\n",
    "First thing we'll need to do is clone [repository](https://github.com/rstrudel/segmenter) containing model and helper functions. We will use Tiny model with mask transformer, that is Seg-T-Mask/16. There are also better, but much larger models available in linked repo. This model is pretrained on [ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/) dataset used for segmentation.\n",
    "\n",
    "PyTorch models are usually instance of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary containing model weights.\n",
    "Typical steps to get model are therefore:\n",
    "\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode\n",
    "\n",
    "In our case, the code from repository already contains functions that create model and load weights, but we will need to download config and trained wight (checkpoint) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need timm and mmsegmentation module, to use segmenter repo\n",
    "!pip install timm mmsegmentation einops mmcv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will clone the Segmenter repo and then download weights and config for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone Segmenter repo\n",
    "if not Path(\"segmenter\").exists():\n",
    "    !git clone https://github.com/rstrudel/segmenter\n",
    "else:\n",
    "    print(\"Segmenter repo already cloned\")\n",
    "\n",
    "%cd segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download config and pretrained model weights\n",
    "# here we use tiny model, there are also better but larger models available in repository\n",
    "WEIGHTS_LINK = \"https://www.rocq.inria.fr/cluster-willow/rstrudel/segmenter/checkpoints/ade20k/seg_tiny_mask/checkpoint.pth\"\n",
    "CONFIG_LINK = \"https://www.rocq.inria.fr/cluster-willow/rstrudel/segmenter/checkpoints/ade20k/seg_tiny_mask/variant.yml\"\n",
    "\n",
    "MODEL_DIR = Path(\"../model/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(WEIGHTS_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "download_file(CONFIG_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "\n",
    "WEIGHT_PATH = MODEL_DIR / \"checkpoint.pth\"\n",
    "CONFIG_PATH = MODEL_DIR / \"variant.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions for preprocessing and visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from segm.data.utils import dataset_cat_description, seg_to_rgb\n",
    "from segm.data.ade20k import ADE20K_CATS_PATH\n",
    "\n",
    "\n",
    "def preprocess(im: Image, normalization: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Preprocess image: scale, normalize and unsqueeze\n",
    "\n",
    "    :param im: input image\n",
    "    :param normalization: dictionary containing normalization data from config file\n",
    "    :return:\n",
    "            im: processed (scaled and normalized) image\n",
    "    \"\"\"\n",
    "    # change PIL image to tensor and scale to [0, 1]\n",
    "    im = F.pil_to_tensor(im).float() / 255\n",
    "    # normalize by given mean and standard deviation\n",
    "    im = F.normalize(im, normalization[\"mean\"], normalization[\"std\"])\n",
    "    # change dim from [C, H, W] to [1, C, H, W]\n",
    "    im = im.unsqueeze(0)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def visualise_mask(pil_im: Image, results: torch.Tensor) -> Image:\n",
    "    \"\"\"\n",
    "    Visualize segmentation masks combined with the image\n",
    "\n",
    "    :param pil_im: original input image\n",
    "    :param results: tensor containing segmentation masks for each pixel\n",
    "    :return:\n",
    "            pil_blend: image with colored segmentation masks overlay\n",
    "    \"\"\"\n",
    "    cat_names, cat_colors = dataset_cat_description(ADE20K_CATS_PATH)\n",
    "\n",
    "    # 3D array, where each pixel has values for all classes, take index of max as label\n",
    "    seg_map = results.argmax(0, keepdim=True)\n",
    "    seg_rgb = seg_to_rgb(seg_map, cat_colors)\n",
    "    seg_rgb = (255 * seg_rgb.cpu().numpy()).astype(np.uint8)\n",
    "    pil_seg = Image.fromarray(seg_rgb[0])\n",
    "\n",
    "    # overlay segmentation mask over original image\n",
    "    pil_blend = Image.blend(pil_im, pil_seg, 0.5).convert(\"RGB\")\n",
    "\n",
    "    return pil_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PyTorch model\n",
    "\n",
    "We will now use already provided helper functions from repository to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segm.model.factory import load_model\n",
    "\n",
    "pytorch_model, config = load_model(WEIGHT_PATH)\n",
    "# put model into eval mode, to set it for inference\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load normalization settings from config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segm.data.utils import STATS\n",
    "\n",
    "normalization_name = config[\"dataset_kwargs\"][\"normalization\"]\n",
    "normalization = STATS[normalization_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check inference of original model\n",
    "\n",
    "We will perform segmentation on example image `coco_hollywood.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segm.model.utils import inference\n",
    "\n",
    "# load image with PIL\n",
    "pil_image = Image.open(\"../../data/image/coco_hollywood.jpg\")\n",
    "\n",
    "# preprocess image with normalization params loaded in cell above\n",
    "image = preprocess(pil_image, normalization)\n",
    "\n",
    "# inference function needs some meta parameters, where we specify that we don't flip images in inference mode\n",
    "im_meta = dict(flip=False)\n",
    "# perform inference with function from repository\n",
    "original_results = inference(model=pytorch_model,\n",
    "                             ims=[image],\n",
    "                             ims_metas=[im_meta],\n",
    "                             ori_shape=image.shape[2:4],\n",
    "                             window_size=config[\"inference_kwargs\"][\"window_size\"],\n",
    "                             window_stride=config[\"inference_kwargs\"][\"window_stride\"],\n",
    "                             batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inference is complete, we need to transform output to segmentation mask where each class has specified color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "blended_image = visualise_mask(pil_image, original_results)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "blended_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model segments the image into meaningful parts.\n",
    "Since we are using tiny variant of model, the result is not as good as it is with larger models, but it already shows nice segmentation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export to ONNX\n",
    "\n",
    "Now that we've verified that inference of PyTorch model works, we will first export it to ONNX format.\n",
    "\n",
    "To do this, we first get input dimensions from model configuration file and create torch dummy input containing input dimensions.\n",
    "After that we use export function from PyTorch to convert model to ONNX.\n",
    "The process can generate some warnings, but they are not a problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# get input sizes from config file\n",
    "image_size = config[\"dataset_kwargs\"][\"image_size\"]\n",
    "batch_size = config[\"dataset_kwargs\"][\"batch_size\"]\n",
    "channels = 3\n",
    "\n",
    "# make dummy input with correct shapes obtained from config file\n",
    "dummy_input = torch.randn(batch_size, channels, image_size, image_size)\n",
    "\n",
    "onnx_path = MODEL_DIR / \"segmenter.onnx\"\n",
    "\n",
    "# export to onnx format\n",
    "torch.onnx.export(pytorch_model,\n",
    "                  dummy_input,\n",
    "                  onnx_path,\n",
    "                  input_names=['input'],                        # the model's input names\n",
    "                  output_names=['output'],                      # the model's output names\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},     # variable length bach size\n",
    "                                'output': {0: 'batch_size'}})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert ONNX model to OpenVINO Intermediate Representation (IR)\n",
    "\n",
    "While ONNX models are directly supported by OpenVINO runtime, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    " `mo.convert_model` function can be used for converting model using OpenVINO Model Optimizer.\n",
    " The function returns instance of OpenVINO Model class, which is ready to use in Python interface but can also be serialized to OpenVINO IR format for future execution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "model = mo.convert_model(str(MODEL_DIR / \"segmenter.onnx\"))\n",
    "# serialize model for saving IR\n",
    "serialize(model, str(MODEL_DIR / \"segmenter.xml\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verify converted model inference\n",
    "\n",
    "To test that model was successfully converted, we can use same inference function from original repository, but we need to make custom class.\n",
    "\n",
    "`SegmenterOV` class contains OpenVINO model, with all attributes and methods required by inference function.\n",
    "This way we don't need to write any additional custom code required to process input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "\n",
    "class SegmenterOV:\n",
    "    \"\"\"\n",
    "    Class containing OpenVINO model with all attributes required to work with inference function.\n",
    "\n",
    "    :param model: compiled OpenVINO model\n",
    "    :type model: CompiledModel\n",
    "    :param output_blob: output blob used in inference\n",
    "    :type output_blob: ConstOutput\n",
    "    :param config: config file containing data about model and its requirements\n",
    "    :type config: dict\n",
    "    :param n_cls: number of classes to be predicted\n",
    "    :type n_cls: int\n",
    "    :param patch_size: size patches that images is split into\n",
    "    :type patch_size: int\n",
    "    :param normalization:\n",
    "    :type normalization: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: Path):\n",
    "        \"\"\"\n",
    "        Constructor method.\n",
    "        Initializes OpenVINO model and sets all required attributes\n",
    "\n",
    "        :param model_path: path to model's .xml file, also containing variant.yml\n",
    "        \"\"\"\n",
    "        # init OpenVino core\n",
    "        core = Core()\n",
    "        # read model\n",
    "        model_xml = core.read_model(model_path)\n",
    "        self.model = core.compile_model(model_xml, 'CPU')\n",
    "        self.output_blob = self.model.output(0)\n",
    "\n",
    "        # load model configs\n",
    "        variant_path = Path(model_path).parent / \"variant.yml\"\n",
    "        with open(variant_path, \"r\") as f:\n",
    "            self.config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # load normalization specs from config\n",
    "        normalization_name = self.config[\"dataset_kwargs\"][\"normalization\"]\n",
    "        self.normalization = STATS[normalization_name]\n",
    "\n",
    "        # load number of classes from config\n",
    "        self.n_cls = self.config[\"net_kwargs\"][\"n_cls\"]\n",
    "\n",
    "        # load patch size from config\n",
    "        self.patch_size = self.config[\"net_kwargs\"][\"patch_size\"]\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform inference on data and return the result in Tensor format\n",
    "\n",
    "        :param data: input data to model\n",
    "        :return: data inferred by model\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(self.model(data)[self.output_blob])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have created SegmenterOV helper class, we can use it in inference function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load model into SegmenterOV class\n",
    "model = SegmenterOV(MODEL_DIR / \"segmenter.xml\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perform inference with same function as in case of PyTorch model from repository\n",
    "results = inference(model=model,\n",
    "                    ims=[image],\n",
    "                    ims_metas=[im_meta],\n",
    "                    ori_shape=image.shape[2:4],\n",
    "                    window_size=model.config[\"inference_kwargs\"][\"window_size\"],\n",
    "                    window_stride=model.config[\"inference_kwargs\"][\"window_stride\"],\n",
    "                    batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "converted_blend = visualise_mask(pil_image, results)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "converted_blend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, we get the same results as with original model."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
